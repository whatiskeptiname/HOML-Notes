{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503980f5",
   "metadata": {},
   "source": [
    "Create a *fetch_housing_data* function to download dataset with ease "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile # for unzipping the dataset\n",
    "import urllib\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    print(\"Data downloaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ae414",
   "metadata": {},
   "source": [
    "Fetch the data by calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7f1bc",
   "metadata": {},
   "source": [
    "Import pandas and create *load_housing_data()* funciton to load data as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007b63a",
   "metadata": {},
   "source": [
    "Create *housing* dataframe and display the first five rows using *head()* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28845676",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()\n",
    "\n",
    "housing.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6635dd",
   "metadata": {},
   "source": [
    "Get the quick description of data using *info()* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a28292",
   "metadata": {},
   "source": [
    "Find out what categories exist and how many districts belong to each category using *value_counts()* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2559a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a030f9d5",
   "metadata": {},
   "source": [
    "View the summary of the numerical attributes using *describe()* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7648132",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7bf59",
   "metadata": {},
   "source": [
    "Importing matplotlib and plot the histogram of the housing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d56d85",
   "metadata": {},
   "source": [
    "Add *split_train_test()* function to split the data into train and test set given data and test_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a1959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d2a948",
   "metadata": {},
   "source": [
    "Save test data to *test_set* and train data to *train_set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_Set = split_train_test(housing, 0.2)\n",
    "len(train_set)\n",
    "len(test_Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac75be3",
   "metadata": {},
   "source": [
    "To make the test and train set consistent we use hash function to seperate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4124c9",
   "metadata": {},
   "source": [
    "Create *split_train_test()* function to split the data by id using hash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af15e39",
   "metadata": {},
   "source": [
    "Add index column to the train and test set to use *split_train_test_by_id()* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd88c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id = housing.reset_index() # adds an `index` column\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf86e1",
   "metadata": {},
   "source": [
    "Combine housing latitude and longitude to create a unique identifier \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b24609",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a60f2b",
   "metadata": {},
   "source": [
    "Use sklearn's train_test_split function to split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4131590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_Set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d34aba",
   "metadata": {},
   "source": [
    "*pd.cut()* function is used to divide the income data into five class labeld from 1 to 5: catagory 1 ranges from 0 to 1.5 (ie., less than $15,000) , catagory 2 from 1.5 to 3 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                                bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                                labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e0e25",
   "metadata": {},
   "source": [
    "Plot the histogram of *income_cat* column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90db7f",
   "metadata": {},
   "source": [
    "Import  StratifiedKFold from sklearn.model_selection to stratify the train set.\n",
    "Strarificaton will eliminate sampling bias in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0faef82",
   "metadata": {},
   "source": [
    "Take a look at income category proportions in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85aeba",
   "metadata": {},
   "source": [
    "Remove the *income_cat* attribute so the data is back to original state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778405f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af713050",
   "metadata": {},
   "source": [
    "Copy the *strat_train_set* dataframe to visualize the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd53fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc27858",
   "metadata": {},
   "source": [
    "Visualize the geographical distribution of the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be913466",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77807f",
   "metadata": {},
   "source": [
    "Use the allpha variable to visualize the density of the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ea178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982cdfdb",
   "metadata": {},
   "source": [
    "The radius of each circle representsthe district’s population (option s), and the color represents the price (option c). Wewill  use  a  predefined  color  map  (option  cmap)  called  jet,  which  ranges  from  blue(low values) to red (high prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "                s=housing[\"population\"]/100, label=\"population\",figsize=(10,7),\n",
    "                c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc538e",
   "metadata": {},
   "source": [
    "Since  the  dataset  is  not  too  large,  you  can  easily  compute  the  standard  correlationcoefficient  (also  called  Pearson’s  r)  between  every  pair  of  attributes  using  the  *corr()*method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd798ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1865f6",
   "metadata": {},
   "source": [
    "Now let’s look at how much each attribute correlates with the median house value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfcb5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49efd6d2",
   "metadata": {},
   "source": [
    "Import *scatter_matrix* from pandas to plot the correlation matrix.\n",
    "This scatter matrix plots every numerical attribute against every othernumerical attribute, plus a histogram of each numerical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "                \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cecfd6c",
   "metadata": {},
   "source": [
    "lksadjflkj aslkdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e6923f",
   "metadata": {},
   "source": [
    "Plot the correlation between *median_income* and *mdian_house_value*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74752d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "                alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c623852",
   "metadata": {},
   "source": [
    "Create new attributes by combining the available ones that are more likely to have signigicant effect on the required output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ccbc5",
   "metadata": {},
   "source": [
    "And now let's look at the correlation matrix again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ebae8",
   "metadata": {},
   "source": [
    "let’s revert to a clean training set (by copying strat_train_set once again).Let’s  also  separate  the  predictors  and  the  labels,  since  we  don’t  necessarily  want  to apply  the  same  transformations  to  the  predictors  and  the  target  values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2141e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "print(housing_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2a98d",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "We can either\n",
    "1. Get rid of the corresponding districts    \n",
    "1. Get rid of the whole attribute\n",
    "1. Set the values to some value (zero, the mean, the median, \n",
    "    etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f714f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.dropna(subset=[\"total_bedrooms\"])   # option 1\n",
    "housing.drop(\"total_bedrooms\", axis=1)      # option 2\n",
    "median = housing[\"total_bedrooms\"].median() # option 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4302e7",
   "metadata": {},
   "source": [
    "Using *SimpleImputer* from *sklearn* create a *SimpleImputer* instance, specifyingthat  you  want  to  replace  each  attribute’s  missing  values  with  the  median  of  that attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e98240",
   "metadata": {},
   "source": [
    "Since the median can only be computed on numerical attributes, you need to create a copy of the data without the text attribute ocean_proximity and use the *fit* method to automatically fit all the na variables with the respective column(Attribute) median values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10fc98c",
   "metadata": {},
   "source": [
    "Now  you  can  use  this  “trained”  imputer  to  transform  the  training  set  by  replacingmissing values with the learned medians. *transform* method now transforms the *housing_num* data using the previously calculated median usingn *housing_num* using the *fit()* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ff3ffe",
   "metadata": {},
   "source": [
    "Transform the plain Numpy array to pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb2834",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159f404",
   "metadata": {},
   "source": [
    "## Handeling Text and Categorical Attributes\n",
    "\n",
    "So  far  we  have  only  dealt  with  numerical  attributes,  but  now  let’s  look  at  textattributes. In this dataset, there is just one: the ocean_proximity attribute. Let’s lookat its value for the first 10 instances:Prepare the Data for Machine Learning Algorithms | 65\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b023b",
   "metadata": {},
   "source": [
    "Let's convert these categories from text to numbers, for this we can use Scikit-Learn's *OrdinalEncoder* Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94898e",
   "metadata": {},
   "source": [
    "Get the list of categories using the *categories_* instance variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161478a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d4e08",
   "metadata": {},
   "source": [
    "Using one hot encoding to encode five classes of proximity to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380539c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f098e25",
   "metadata": {},
   "source": [
    "Convert the SciPy sparse matrix to NumPy array (Although it is not compulsory). The space occupied by Scipy Sparse matrix is less than NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65604b",
   "metadata": {},
   "source": [
    "## Custom Transformers\n",
    "\n",
    "Although  Scikit-Learn  provides  many  useful  transformers,  you  will  need  to  writeyour   own   for   tasks   such   as   custom   cleanup   operations   or   combining   specificattributes. Scikit-Learn uses duck typing (not inheritance), all  you  need  to  do  is  create  a  class  and  implement  three  methods:  fit()(returning self), transform(), and fit_transform(). You can get the last one for free by simply adding TransformerMixin as a base class.If you add BaseEstimator as a base class (and avoid &ast;args and &ast;&ast;kargs in your constructor), you will also get two extra methods (get_params() and set_params()) thatwill be useful for automatic hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68e7cc",
   "metadata": {},
   "source": [
    "For  example,  here  is  a  small  transformer  class  that  adds  the  combined  attributes  wediscussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93dec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3,4,5,6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_atrribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89790ae",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "One of the most important transformations you need to apply to your data is featurescaling. With few exceptions, Machine Learning algorithms don’t perform well whenthe input numerical attributes have very different scales. This is the case for the hous‐ing data: the total number of rooms ranges from about 6 to 39,320, while the medianincomes  only  range  from  0  to  15.  Note  that  scaling  the  target  values  is  generally  notrequired.</br>\n",
    "There  are  two  common  ways  to  get  all  attributes  to  have  the  same  scale:min-maxscaling and standardization.</br>\n",
    "\n",
    "**Min-max scaling** (many people call this normalization) is the simplest: values are shif‐ted  and  rescaled  so  that  they  end  up  ranging  from  0  to  1.  We  do  this  by  subtractingthe min value and dividing by the max minus the min. Scikit-Learn provides a trans‐former called MinMaxScaler for this. It has a feature_range hyperparameter that letsyou change the range if, for some reason, you don’t want 0–1.</br>\n",
    "\n",
    "**Standardization**  is  different:  first  it  subtracts  the  mean  value  (so  standardized  valuesalways  have  a  zero  mean),  and  then  it  divides  by  the  standard  deviation  so  that  theresulting  distribution  has  unit  variance.  Unlike  min-max  scaling,  standardizationdoes  not  bound  values  to  a  specific  range,  which  may  be  a  problem  for  some  algo‐rithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐ever, standardization is much less affected by outliers. For example, suppose a districthad a median income equal to 100 (by mistake). Min-max scaling would then crushall the other values from 0–15 down to 0–0.15, whereas standardization would not bemuch   affected.   Scikit-Learn   provides   a   transformer   called   StandardScaler   forstandardization.</br>\n",
    "As  with  all  the  transformations,  it  is  important  to  fit  the  scalers  tothe training data only, not to the full dataset (including the test set).Only  then  can  you  use  them  to  transform  the  training  set  and  thetest set (and new data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8207fb59",
   "metadata": {},
   "source": [
    "## Transformation Pipelines\n",
    "\n",
    "As you can see, there are many data transformation steps that need to be executed inthe  right  order.  Fortunately,  Scikit-Learn  provides  the  Pipeline  class  to  help  withsuch  sequences  of  transformations.  Here  is  a  small  pipeline  for  the  numericalattributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d1ef8",
   "metadata": {},
   "source": [
    "The Pipeline constructor takes a list of name/estimator pairs defining a sequence ofsteps.  All  but  the  last  estimator  must  be  transformers  (i.e.,  they  must  have  a  fit_transform() method). The names can be anything you like (as long as they are unique  and  don’t  contain  double  underscores. they  will  come  in  handy  later  forhyperparameter tuning.</br>\n",
    "\n",
    "When you call the pipeline’s fit() method, it calls fit_transform() sequentially on all transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it calls the fit() method.</br>\n",
    "\n",
    "The pipeline exposes the same methods as the final estimator. In this example, the last estimator  is  a  StandardScaler,  which  is  a  transformer,  so  the  pipeline  has  a  transform() method that applies all the transforms to the data in sequence (and of course also a fit_transform() method, which is the one we used).\n",
    "</br>\n",
    "\n",
    "So  far,  we  have  handled  the  categorical  columns  and  the  numerical  columns  separately. It would be more convenient to have a single transformer able to handle all columns, applying  the  appropriate  transformations  to  each  column.  In  version  0.20, Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news is that it works great with pandas DataFrames. Let’s use it to apply all the transforma‐tions to the housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91080888",
   "metadata": {},
   "source": [
    "## Select and Train a Model\n",
    "\n",
    "At  last!  You  framed  the  problem,  you  got  the  data  and  explored  it,  you  sampled  a training  set  and  a  test  set,  and  you  wrote  transformation  pipelines  to  clean  up  and prepare your data for Machine Learning algorithms automatically. You are now ready to select and train a Machine Learning model.</br>\n",
    "\n",
    "### Training and Evaluating on the Training Set\n",
    "\n",
    "Thanks to all the previous steps, things are now going to be much simpler. Let's first train a Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cfca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b2eac",
   "metadata": {},
   "source": [
    "Done! Now we have a working Linear Regression model. Let's try it out on a few instances from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9a83d",
   "metadata": {},
   "source": [
    "It works, although the predictions are not exactly accurate (e.g., the first prediction is off by close to 40%!). Let’s measure this regression model’s RMSE on the whole training set using Scikit-Learn’s mean_squared_error() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69983bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5636c2",
   "metadata": {},
   "source": [
    "The above *lin_rmse* value is not satisfactory, so we now use more powerful algorithm to predict the housing price. Let’s  train  a  DecisionTreeRegressor.  This  is  a  powerful  model,  capable  of  findingcomplex  nonlinear  relationships  in  the  data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc52c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8da11",
   "metadata": {},
   "source": [
    "Now that the model is trained, let's evaluate it on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18dad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e7abe",
   "metadata": {},
   "source": [
    "Looking at the value of *tree_rmse* which is zero, it is more likely that the model has bsdly overfit the data. So for the better evaluation we will be using Cross-Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120bd0e9",
   "metadata": {},
   "source": [
    "### Better Evaluation Using Cross-Validation\n",
    "\n",
    "One    way    to    evaluate    the    Decision    Tree    model    would    be    to    use    thetrain_test_split() function to split the training set into a smaller training set and avalidation  set,  then  train  your  models  against  the  smaller  training  set  and  evaluatethem  against  the  validation  set.  It’s  a  bit  of  work,  but  nothing  too  difficult,  and  itwould work fairly well </br>\n",
    "\n",
    "A great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The following  code  randomly  splits  the  training  set  into  10  distinct  subsets  called  folds,  then  it trains  and  evaluates  the  Decision  Tree  model  10  times,  picking  a  different  fold  for evaluation  every  time  and  training  on  the  other  9  folds.  The  result  is  an  array  con‐taining the 10 evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared,\n",
    "                         housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv=10\n",
    "                        )\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "tree_rmse_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832ad0b",
   "metadata": {},
   "source": [
    "Scikit-Learn’s  cross-validation  features  expect  a  utility  function (greater  is  better) rather  than  a  cost  function  (lower  is  better), so the scoring function is actually the opposite of the MSE (i.e., a neg‐ative  value),  which  is  why  the  preceding  code  computes  -scores before calculating the square root. </br>\n",
    "create a function to display the rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores, \"\\n\")\n",
    "    print(\"Mean:\", scores.mean(), \"\\n\")\n",
    "    print(\"Standard Deviation:\", scores.std(), \"\\n\")\n",
    "    \n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9157b7",
   "metadata": {},
   "source": [
    "Let’s compute the same scores for the Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared,\n",
    "                            housing_labels,\n",
    "                            scoring=\"neg_mean_squared_error\",\n",
    "                            cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5bb73f",
   "metadata": {},
   "source": [
    "The Decision Tree model is overfitting so badly that it performs worse that the Linear Regression model.</br>\n",
    "\n",
    "Let's try one last model now: **The RandomForestRegressor**. Random  Forests  work  by  training  many  Decision  Trees  on random subsets of the features, then averaging out their predictions. Building  a  model  on  top  of  many other models is called Ensemble Learning, and it is often a great way to push ML algo‐rithms  even  further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e94772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "forest_reg_predicitons = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, forest_reg_predicitons) \n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print(\"rmse without cv:\", forest_rmse, \"\\n\")\n",
    "\n",
    "forest_mse_scores = cross_val_score(forest_reg, housing_prepared,\n",
    "                                housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\",\n",
    "                                cv=10\n",
    "                               )\n",
    "\n",
    "forest_rmse_scores = np.sqrt(-forest_mse_scores)\n",
    "display_scores(forest_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dea4f1",
   "metadata": {},
   "source": [
    "The above model is performing better than pervious models but the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. Possible solutions for overfitting are to simplify the model, constrain it .(i.e., regularize it), or get a lot more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4afd8",
   "metadata": {},
   "source": [
    "### Saving The Model\n",
    "\n",
    "We  can  easily  save Scikit-Learn  models  by  using Python’s  **pickle  module**  or  by using the **joblib library**,  which  is  more  efficient  at  serializing  largeNumPy arrays (you can install this library using pip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb6ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "#joblib.dump(forest_reg, \"RandomForestRegression.pkl\")\n",
    "\n",
    "# Load the saved model\n",
    "#my_model_loaded = joblib.load(\"RandomForestRegression.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9b08adf87d7192cfe942781fd54415438870c456d89c858e0a76f55356c9d48"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python396jvsc74a57bd0c9b08adf87d7192cfe942781fd54415438870c456d89c858e0a76f55356c9d48"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
