{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02996c2c",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf077c",
   "metadata": {},
   "source": [
    "Many Machine Learning problems involve thousands or even millions of features foreach  training  instance.  Not  only  do  all  these  features  make  training  extremely  slow,but  they  can  also  make  it  much  harder  to  find  a  good  solution,  as  we  will  see. Thisproblem is often referred to as the curse of dimensionality.\n",
    "\n",
    "Fortunately, in realworld problems, it is often possible to reduce the number of features considerably, turning an intractable problem into a tractable one. For example,consider the MNIST images: the pixels on the image borders  are  almost  always  white,  so  you  could  completely  drop  these  pixels  from  the training  set  without  losing  much  information. Additionally, two neighboring pixels  are  often  highly  correlated:  if  you  merge  them  into  a  single  pixel  (e.g.,  by  taking the mean of the two pixel intensities), you will not lose much information.\n",
    "\n",
    "Reducing  dimensionality  does  cause  some  information  loss  (just like  compressing  an  image  to  JPEG  can  degrade  its  quality),  soeven  though  it  will  speed  up  training,  it  may  make  your  system perform  slightly  worse.  It  also  makes  your  pipelines  a  bit  more complex  and  thus  harder  to  maintain.  So,  if  training  is  too  slow, you  should  first  try  to  train  your  system  with  the  original  data before  considering  using  dimensionality  reduction.  In  some  cases, reducing  the  dimensionality  of  the  training  data  may  filter  outsome  noise  and  unnecessary  details  and  thus  result  in  higher  performance, but in general it wonâ€™t; it will just speed up training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a90f4c",
   "metadata": {},
   "source": [
    "Apart  from  speeding  up  training,  dimensionality  reduction  is  also  extremely  usefulfor data visualization (or DataViz). Reducing the number of dimensions down to two(or three) makes it possible to plot a condensed view of a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such  as  clusters.  Moreover,  DataViz  is  essential  to  communicate  your  conclusions  to people who are not data scientists in particular, decision makers who will use your results.In  this  chapter  we  will  discuss  the  curse  of  dimensionality  and  get  a  sense  of  whatgoes on in high-dimensional space. Then, we will consider the two main approachesto  dimensionality  reduction  (projection  and  Manifold  Learning),  and  we  will  gothrough three of the most popular dimensionality reduction techniques: PCA, KernelPCA, and LLE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python396jvsc74a57bd0c9b08adf87d7192cfe942781fd54415438870c456d89c858e0a76f55356c9d48"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
